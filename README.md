**Progressive Transformers Scratchpad**

- Goal: learn and build up to transformer models by training a simple baseline (“current”) and a more expressive “challenger” model side‑by‑side on progressively harder toy tasks.
- Status: trains two models per run and logs results to W&B with per‑model namespaces.
- Disclaimer: this README was generated by an AI assistant.

**How It Works**

- Data: generated in `data.py` and imported in `training.py`. Inputs have `F` features per timestep `(N, T, F)`. Task labels are computed on the per‑timestep sum over features:
  - `sign_of_winner`: `y=1` if the argmax of `|sum_t|` is positive.
  - `has_pos_and_neg` (default): `y=1` if the summed sequence contains at least one positive and one negative value.
- Models (train each run):
  - `logreg`: logistic regression over the flattened sequence of `F` features (linear baseline).
  - `self_attention`: simple self‑attention over timesteps with learned projection and row‑wise softmax over keys.
  - `attention`: learned attention pooling over timesteps producing a `(N, 1)` probability.
- Training loop: shared for both models; logs per‑epoch metrics and periodic distributions via a unified callback.
- Reporting: end‑of‑run evaluation panels (ROC/PR/confusion) are created for both models with separate prefixes.

**Repo Structure**

- `main.py`
  - Orchestrates a run, sets W&B settings, defines per‑model step metrics, and supplies a unified logger callback.
  - Calls `run_training(...)` and then `generate_run_report(...)` for each model under `logreg/`, `self_attention/`, and `attention/` prefixes.
- `models.py`
  - Model definitions: `SimpleTemporalPoolingClassifier`, `AttentionPoolingClassifier`.
  - Builders: `build_logreg(sequence_length)`, `build_model(sequence_length)`.
  - Access wrappers: `LogRegAccess`, `TemporalAccess`, `AttentionAccess` unify the training interface and expose the final Linear for diagnostics.
- `training.py`
  - Imports `prepare_data(...)` and the access wrappers from `models.py`.
  - `train_model(model: ModelAccess, x, y, on_log, hist_every)`: shared loop; epochs and learning rate come from the `model`.
  - `run_training(...)`: prepares data once, trains three models, returns artifacts as `{"logreg": ..., "self_attention": ..., "attention": ...}`.
- `report.py`
  - `generate_run_report(run, artifacts, prefix="")`: logs W&B panels (metrics histories, ROC/PR/confusion) under the provided prefix.
- `pyproject.toml`: dependencies (torch, wandb, numpy).

**Run It**

- Prereqs: Python 3.8+, Torch, W&B account or offline mode.
- Auth: the code does not contain credentials; run `wandb login` once or set `WANDB_API_KEY`.
- Start training: `python main.py`
  - W&B charts appear under `logreg/*`, `self_attention/*`, and `attention/*` namespaces.
- Offline: `WANDB_MODE=offline python main.py` (logs saved locally and can be synced later).

**What Gets Logged**

- Per‑epoch scalars per model: `metrics/loss`, `metrics/accuracy`, `metrics/grad_norm`, `metrics/weight_norm`.
- Distributions every `hist_every` epochs: probabilities and logits histograms.
- End‑of‑run eval per model: ROC curve, PR curve, confusion matrix.
- W&B steps: model‑specific step metrics (`logreg/step`, `self_attention/step`, `attention/step`) avoid global step collisions.

**Progression Plan (Curriculum)**

- Start simple (linearly separable): trend sign, last > first.
- Non‑linear signals: XOR of endpoints; “any x > 0”; mixed‑rule scoring (current); “majority > 0”; parity of sign changes; max/min positions.
- Add positional information: absolute/relative position features; learned positional embeddings.
- Increase expressiveness of the challenger:
  - Per‑timestep projection + pooling.
  - Attention pooling with a learned query (implemented).
  - Single‑head self‑attention block over timesteps.
  - Multi‑head self‑attention + residual + layer norm.
  - Transformer encoder layer(s) with positional encodings.
- Keep the `logreg` baseline for contrast; require the challenger to outperform as tasks harden.

**Extending**

- Add a new challenger: implement a builder that returns a `torch.nn.Module` producing probabilities `(N, 1)` and wire it into `run_training`.
- Add tasks: tweak `prepare_data` to generate new targets; keep difficulty incremental.
- Tuning:
  - Adjust `SEQUENCE_LENGTH`, `N_SAMPLES`, and `hist_every` in `main.py`.
  - Adjust `epochs` and `lr` in the access wrappers in `models.py` (or extend the code to pass them from `main.py`).

**Notes**

- System metrics are disabled via `wandb.Settings(_disable_stats=True)`; only your training metrics are logged.
- If you see step ordering warnings, ensure you’re using the per‑model step metrics (already configured in `main.py`).
