**Progressive Transformers Scratchpad**

- Goal: learn and build up to transformer models by training a simple baseline (“current”) and a more expressive “challenger” model side‑by‑side on progressively harder toy tasks.
- Status: trains two models per run and logs results to W&B with per‑model namespaces.
- Disclaimer: this README was generated by an AI assistant.

**How It Works**

- Data: generated on the fly in `training.py`. Current target is “any x > 0” with about 50% positives and 10% label noise.
- Models (both train each run):
  - `logreg`: logistic regression over the flattened sequence (linear baseline).
  - `temporal`: simple per‑timestep projection, mean over features to `(N, T)`, then a classifier over timesteps (a small step toward sequence modeling).
- Training loop: shared for both models; logs per‑epoch metrics and periodic distributions via a unified callback.
- Reporting: end‑of‑run evaluation panels (ROC/PR/confusion) are created for both models with separate prefixes.

**Repo Structure**

- `main.py`
  - Orchestrates a run, sets W&B settings, defines per‑model step metrics, and supplies a unified logger callback.
  - Calls `run_training(...)` and then `generate_run_report(...)` for each model under `logreg/` and `temporal/` prefixes.
- `training.py`
  - `prepare_data(sequence_length, n_samples, seed)`: creates sequences with skewed normals so that `P(any x > 0) ≈ 0.5`, then injects 10% label noise.
  - `build_logreg(sequence_length)`: linear baseline.
  - `build_model(sequence_length)`: simple temporal pooling classifier.
  - `train_model(model, x, y, epochs, learning_rate, model_name, on_log, hist_every)`: shared loop; returns artifacts.
  - `run_training(...)`: prepares data once, trains both models, returns artifacts as `{"logreg": ..., "temporal": ...}`.
- `report.py`
  - `generate_run_report(run, artifacts, prefix="")`: logs W&B panels (metrics histories, ROC/PR/confusion, distributions) under the provided prefix.
- `pyproject.toml`: dependencies (torch, wandb, numpy, pandas, matplotlib).

**Run It**

- Prereqs: Python 3.8+, Torch, W&B account or offline mode.
- Auth: the code does not contain credentials; run `wandb login` once or set `WANDB_API_KEY`.
- Start training: `python main.py`
  - W&B charts appear under `logreg/*` and `temporal/*` namespaces.
- Offline: `WANDB_MODE=offline python main.py` (logs saved locally and can be synced later).

**What Gets Logged**

- Per‑epoch scalars per model: `metrics/loss`, `metrics/accuracy`, `metrics/grad_norm`, `metrics/weight_norm`, `metrics/bias_abs`.
- Distributions every `hist_every` epochs: probabilities and logits histograms.
- End‑of‑run eval per model: ROC curve, PR curve, confusion matrix.
- W&B steps: model‑specific step metrics (`logreg/step`, `temporal/step`) avoid global step collisions.

**Progression Plan (Curriculum)**

- Start simple (linearly separable): trend sign, last > first.
- Non‑linear signals: XOR of endpoints; “any x > 0” (current); “majority > 0”; parity of sign changes; max/min positions.
- Add positional information: absolute/relative position features; learned positional embeddings.
- Increase expressiveness of the challenger:
  - Per‑timestep projection + pooling (current).
  - Attention pooling with a learned query.
  - Single‑head self‑attention block over timesteps.
  - Multi‑head self‑attention + residual + layer norm.
  - Transformer encoder layer(s) with positional encodings.
- Keep the `logreg` baseline for contrast; require the challenger to outperform as tasks harden.

**Extending**

- Add a new challenger: implement a builder that returns a `torch.nn.Module` producing probabilities `(N, 1)` and wire it into `run_training`.
- Add tasks: tweak `prepare_data` to generate new targets; keep difficulty incremental.
- Tuning: adjust `EPOCHS`, `LEARNING_RATE`, and `hist_every` in `main.py`.

**Notes**

- System metrics are disabled via `wandb.Settings(_disable_stats=True)`; only your training metrics are logged.
- If you see step ordering warnings, ensure you’re using the per‑model step metrics (already configured in `main.py`).
