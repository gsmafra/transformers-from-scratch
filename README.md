**Progressive Transformers Scratchpad**

- Goal: learn and build up to transformer models by training a simple baseline (“current”) and a more expressive “challenger” model side‑by‑side on progressively harder toy tasks.
- Status: trains two models per run and logs results to W&B with per‑model namespaces.
- Disclaimer: this README was generated by an AI assistant.

**How It Works**

- Data: generated in `data.py` and imported in `training.py`. Current task is the "sign of the winner": for each sequence, find the index of the maximum absolute value and set `y=1` if that winning value is positive, else `0`.
- Models (train each run):
  - `logreg`: logistic regression over the flattened sequence (linear baseline).
  - `temporal`: per‑timestep projection, mean over features to `(N, T)`, then a classifier over timesteps.
  - `attention`: learned attention pooling over timesteps producing a `(N, 1)` probability.
- Training loop: shared for both models; logs per‑epoch metrics and periodic distributions via a unified callback.
- Reporting: end‑of‑run evaluation panels (ROC/PR/confusion) are created for both models with separate prefixes.

**Repo Structure**

- `main.py`
  - Orchestrates a run, sets W&B settings, defines per‑model step metrics, and supplies a unified logger callback.
  - Calls `run_training(...)` and then `generate_run_report(...)` for each model under `logreg/` and `temporal/` prefixes.
- `training.py`
  - Imports `prepare_data(...)` from `data.py` to create sequences for the sign‑of‑the‑winner task.
  - `build_logreg(sequence_length)`: linear baseline.
  - `build_model(sequence_length)`: simple temporal pooling classifier.
  - `ModelAccess` wrappers: `LogRegAccess`, `TemporalAccess`, and `AttentionAccess` unify the training interface and expose the final linear layer for diagnostics.
  - `train_model(model: ModelAccess, x, y, on_log, hist_every)`: shared loop; epochs and learning rate are taken from the `model`.
  - `run_training(...)`: prepares data once, trains three models, returns artifacts as `{"logreg": ..., "temporal": ..., "attention": ...}`.
- `report.py`
  - `generate_run_report(run, artifacts, prefix="")`: logs W&B panels (metrics histories, ROC/PR/confusion) under the provided prefix.
- `pyproject.toml`: dependencies (torch, wandb, numpy).

**Run It**

- Prereqs: Python 3.8+, Torch, W&B account or offline mode.
- Auth: the code does not contain credentials; run `wandb login` once or set `WANDB_API_KEY`.
- Start training: `python main.py`
  - W&B charts appear under `logreg/*`, `temporal/*`, and `attention/*` namespaces.
- Offline: `WANDB_MODE=offline python main.py` (logs saved locally and can be synced later).

**What Gets Logged**

- Per‑epoch scalars per model: `metrics/loss`, `metrics/accuracy`, `metrics/grad_norm`, `metrics/weight_norm`, `metrics/bias_abs`.
- Distributions every `hist_every` epochs: probabilities and logits histograms.
- End‑of‑run eval per model: ROC curve, PR curve, confusion matrix.
- W&B steps: model‑specific step metrics (`logreg/step`, `temporal/step`) avoid global step collisions.

**Progression Plan (Curriculum)**

- Start simple (linearly separable): trend sign, last > first.
- Non‑linear signals: XOR of endpoints; “any x > 0”; mixed‑rule scoring (current); “majority > 0”; parity of sign changes; max/min positions.
- Add positional information: absolute/relative position features; learned positional embeddings.
- Increase expressiveness of the challenger:
  - Per‑timestep projection + pooling.
  - Attention pooling with a learned query (implemented).
  - Single‑head self‑attention block over timesteps.
  - Multi‑head self‑attention + residual + layer norm.
  - Transformer encoder layer(s) with positional encodings.
- Keep the `logreg` baseline for contrast; require the challenger to outperform as tasks harden.

**Extending**

- Add a new challenger: implement a builder that returns a `torch.nn.Module` producing probabilities `(N, 1)` and wire it into `run_training`.
- Add tasks: tweak `prepare_data` to generate new targets; keep difficulty incremental.
- Tuning:
  - Adjust `SEQUENCE_LENGTH`, `N_SAMPLES`, and `hist_every` in `main.py`.
  - Adjust `epochs` and `lr` in the `LogRegAccess`/`TemporalAccess` defaults in `training.py` (or extend the code to pass them from `main.py`).

**Notes**

- System metrics are disabled via `wandb.Settings(_disable_stats=True)`; only your training metrics are logged.
- If you see step ordering warnings, ensure you’re using the per‑model step metrics (already configured in `main.py`).
